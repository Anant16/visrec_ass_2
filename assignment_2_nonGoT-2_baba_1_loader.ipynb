{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#added by me\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "import unicodedata\n",
    "from IPython import display\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "#end of added by me\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# Import other modules if required\n",
    "\n",
    "resnet_input = 224 #size of resnet18 input images\n",
    "use_gpu = True\n",
    "num_of_classes = 21\n",
    "window_batch_size = 20\n",
    "window_threshold = 12\n",
    "epsilon = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 1\n",
    "num_epochs = 5\n",
    "learning_rate =  0.005\n",
    "hyp_momentum = 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomWindow(width, height):\n",
    "    xa = random.randint(0,width)\n",
    "    xb = random.randint(0,width)\n",
    "    ya = random.randint(0,height)\n",
    "    yb = random.randint(0,height)\n",
    "    x1 = min(xa,xb)\n",
    "    x2 = max(xa,xb)\n",
    "    y1 = min(ya,yb)\n",
    "    y2 = max(ya,yb)\n",
    "    return (x1,y1,x2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxiou(rwindow, boxes, threshold=0.2):\n",
    "    minWindowSize = 60\n",
    "    if(rwindow[2] - rwindow[0] < minWindowSize or rwindow[3] - rwindow[1] < minWindowSize ):\n",
    "        return 0\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    xx1 = np.maximum(rwindow[0], x1[:])\n",
    "    yy1 = np.maximum(rwindow[1], y1[:])\n",
    "    xx2 = np.minimum(rwindow[2], x2[:])\n",
    "    yy2 = np.minimum(rwindow[3], y2[:])\n",
    "    # compute the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "    # compute the ratio of overlap\n",
    "    overlap = (w * h) / area[:]\n",
    "    #print(max(overlap))\n",
    "    if(max(overlap) < threshold):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(train, rd, rd_annot):\n",
    "    # Begin\n",
    "    new_map = list()\n",
    "    class_count = np.zeros(num_of_classes)\n",
    "    for subdir, dirs, files in os.walk(rd):\n",
    "        for File in files:\n",
    "            xmlFile = File.split('.')[0] + '.xml'\n",
    "            xmlFileDest = rd_annot + '/' + xmlFile\n",
    "            tree = ET.parse(xmlFileDest)\n",
    "            root = tree.getroot()\n",
    "            sz = root.find('size')\n",
    "            width = int(sz.find('width').text)\n",
    "            height = int(sz.find('height').text)\n",
    "\n",
    "            objinfile = list()\n",
    "            objlist = list()\n",
    "            for obj in root.iter('object'):\n",
    "                objclass = obj.find('name').text\n",
    "                classid = classes.index(objclass)\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = int(bndbox.find('xmin').text)\n",
    "                ymin = int(bndbox.find('ymin').text)\n",
    "                xmax = int(bndbox.find('xmax').text)\n",
    "                ymax = int(bndbox.find('ymax').text)\n",
    "                    \n",
    "                    #if(train == True):\n",
    "                new_map = new_map + [(File, (xmin, ymin, xmax, ymax), classid)] #objclass is a string name of class.\n",
    "                objinfile = objinfile + [(xmin, ymin, xmax, ymax)]\n",
    "                    #else:\n",
    "                        #objlist = objlist + [((xmin, ymin, xmax, ymax), classid)]\n",
    "                class_count[classid]+=1\n",
    "                if(train == True):\n",
    "                    rwindow = getRandomWindow(width, height)\n",
    "                    #print(rwindow)\n",
    "                    if( maxiou(rwindow, np.array(objinfile)) > 0):\n",
    "                        #print(classes[0])\n",
    "                        #add background image\n",
    "                        new_map = new_map + [(File, rwindow, 0)]\n",
    "                        class_count[0]+=1\n",
    "                        #else:\n",
    "                 #   self.map = self.map + [(File, objlist)]\n",
    "    return new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform  = transform\n",
    "        self.map = list()\n",
    "        \n",
    "        if(train == True):\n",
    "            rd = root_dir + \"/VOCdevkit_train/VOC2007/JPEGImages\"\n",
    "            rd_annot = root_dir + \"/VOCdevkit_train/VOC2007/Annotations\"\n",
    "            \n",
    "        else:\n",
    "            rd = root_dir + \"/VOCdevkit_test/VOC2007/JPEGImages\"\n",
    "            rd_annot = root_dir + \"/VOCdevkit_test/VOC2007/Annotations\"\n",
    "        \n",
    "        self.map = build_dataset(train, rd, rd_annot)\n",
    "                \n",
    "                       \n",
    "        #print(\"min height and width\")\n",
    "        #print(h,w)\n",
    "        #print(\"num of examples per class:\")\n",
    "        #print(classes)\n",
    "        #print(class_count)\n",
    "               \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.map)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       # Begin\n",
    "        if(self.train == True):\n",
    "            rd = self.root_dir + \"/VOCdevkit_train/VOC2007/JPEGImages\"\n",
    "            rd_annot = self.root_dir + \"/VOCdevkit_train/VOC2007/Annotations\"\n",
    "            \n",
    "        else:\n",
    "            rd = self.root_dir + \"/VOCdevkit_test/VOC2007/JPEGImages\"\n",
    "            rd_annot = self.root_dir + \"/VOCdevkit_test/VOC2007/Annotations\"\n",
    "            #return (im, self.map[1])\n",
    "        img = Image.open(rd + '/' + self.map[idx][0])\n",
    "        area = self.map[idx][1] #(xmin, ymin, xmax, ymax)\n",
    "        img = img.crop(area)  #taking only the part specified by the (xmin, ymin, xmax, ymax)\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(img)\n",
    "        return (im, self.map[idx][2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_test_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform  = transform\n",
    "        self.map = list()\n",
    "        \n",
    "        if(train == True):\n",
    "            rd = root_dir + \"/VOCdevkit_train/VOC2007/JPEGImages\"\n",
    "            rd_annot = root_dir + \"/VOCdevkit_train/VOC2007/Annotations\"\n",
    "            \n",
    "        else:\n",
    "            rd = root_dir + \"/VOCdevkit_test/VOC2007/JPEGImages\"\n",
    "            rd_annot = root_dir + \"/VOCdevkit_test/VOC2007/Annotations\"\n",
    "        \n",
    "        class_count = np.zeros(num_of_classes)\n",
    "        for subdir, dirs, files in os.walk(rd):\n",
    "            for File in files:\n",
    "                xmlFile = File.split('.')[0] + '.xml'\n",
    "                xmlFileDest = rd_annot + '/' + xmlFile\n",
    "                tree = ET.parse(xmlFileDest)\n",
    "                root = tree.getroot()\n",
    "                sz = root.find('size')\n",
    "                width = int(sz.find('width').text)\n",
    "                height = int(sz.find('height').text)\n",
    "                \n",
    "                objinfile = list()\n",
    "                objlist = list()\n",
    "                for obj in root.iter('object'):\n",
    "                    objclass = obj.find('name').text\n",
    "                    classid = classes.index(objclass)\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    \n",
    "                    if(train == True):\n",
    "                        self.map = self.map + [(File, (xmin, ymin, xmax, ymax), classid)] #objclass is a string name of class.\n",
    "                        objinfile = objinfile + [(xmin, ymin, xmax, ymax)]\n",
    "                    else:\n",
    "                        objlist = objlist + [((xmin, ymin, xmax, ymax), classid)]\n",
    "                    class_count[classid]+=1\n",
    "                if(train == True):\n",
    "                    rwindow = getRandomWindow(width, height)\n",
    "                    #print(rwindow)\n",
    "                    if( maxiou(rwindow, np.array(objinfile)) > 0):\n",
    "                        #print(classes[0])\n",
    "                        #add background image\n",
    "                        self.map = self.map + [(File, rwindow, 0)]\n",
    "                        class_count[0]+=1\n",
    "                else:\n",
    "                    self.map = self.map + [(File, objlist)]\n",
    "                       \n",
    "        #print(\"min height and width\")\n",
    "        #print(h,w)\n",
    "        print(\"num of examples per class:\")\n",
    "        print(classes)\n",
    "        print(class_count)\n",
    "               \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.map)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       # Begin\n",
    "        if(self.train == True):\n",
    "            rd = self.root_dir + \"/VOCdevkit_train/VOC2007/JPEGImages\"\n",
    "            rd_annot = self.root_dir + \"/VOCdevkit_train/VOC2007/Annotations\"\n",
    "            img = Image.open(rd + '/' + self.map[idx][0])\n",
    "            area = self.map[idx][1] #(xmin, ymin, xmax, ymax)\n",
    "            img = img.crop(area)  #taking only the part specified by the (xmin, ymin, xmax, ymax)\n",
    "            if self.transform is not None:\n",
    "                im = self.transform(img)\n",
    "            return (im, self.map[idx][2])\n",
    "        else:\n",
    "            rd = self.root_dir + \"/VOCdevkit_test/VOC2007/JPEGImages\"\n",
    "            rd_annot = self.root_dir + \"/VOCdevkit_test/VOC2007/Annotations\"\n",
    "            img = Image.open(rd + '/' + self.map[idx][0])\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            #print(\"objlist:\")\n",
    "            #print(self.map[1])\n",
    "            return (img, self.map[idx][1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of examples per class:\n",
      "('__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
      "[    0.   311.   389.   576.   393.   657.   254.  1541.   370.  1374.\n",
      "   329.   299.   530.   395.   369.  5227.   592.   311.   396.   302.\n",
      "   361.]\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.ToTensor() ])\n",
    "composed_test_transform = transforms.Compose([\n",
    "                                         transforms.ToTensor() ])\n",
    "revert_transform = transforms.Compose([\n",
    "                                         transforms.ToPILImage() ])\n",
    "#transforms.RandomHorizontalFlip() was removed by me from above.\n",
    "train_dataset = voc_dataset(root_dir='.', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = voc_dataset(root_dir='.', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "test_pipe_dataset = voc_test_dataset(root_dir='.', train=False)#, transform=composed_test_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_pipe_loader = torch.utils.data.DataLoader(dataset=test_pipe_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here\n",
    "if(torch.cuda.is_available() and use_gpu):\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Begin\n",
    "    x = list()\n",
    "    y = list()\n",
    "    trdata_batchsize = len(train_dataset)//batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 256 == 0: \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, trdata_batchsize, loss.data[0]))\n",
    "                x.append((epoch*trdata_batchsize) + 1+i)\n",
    "                y.append(loss.data[0])\n",
    "                plt.plot(x,y,color = 'red')\n",
    "                plt.title('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, trdata_batchsize, loss.data[0]))\n",
    "                plt.xlabel(\"Batch Number\")\n",
    "                plt.ylabel(\"Cross Entropy Loss\")\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "            #if i == 400:\n",
    "             #   break\n",
    "        #break    #to run just 1 epoch\n",
    "    plt.savefig('Loss_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the weights you got by training it earlier.\n",
    "resnet18.load_state_dict(torch.load('assignment2_0005_20_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the weights for future analysis\n",
    "#torch.save(resnet18.state_dict(), 'assignment2_2_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load saved model\n",
    "#resnet18.load_state_dict(torch.load('assignment2_2_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        #doing this on cpu due to gpu memory leak.\n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%time test_accuracy(resnet18)\n",
    "# 62% acc after 5 epochs, lrate = 0.005\n",
    "# 65% acc after 10 epochs, lrate = 0.005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes,threshold = 0.4):\n",
    "    # \n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    " \n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    " \n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    " \n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / (area[idxs[:last]])\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > threshold)[0])))\n",
    " \n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "    #return boxes[pick].astype(\"int\")\n",
    "    #print(pick)\n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(sz):\n",
    "    \n",
    "    # Begin\n",
    "    stride = 20   #subject to alot of change\n",
    "    #for image in images:\n",
    "    #image = revert_transform(image)\n",
    "    #image = transforms.ToPILImage(image)\n",
    "    res = np.array([(50,100),(100,50),(100,100)])/2  #may wish to change to powers of 2.\n",
    "    res = res.astype(int)\n",
    "    #sz = image.size\n",
    "    area_set = list()\n",
    "    #cropped_images = list()\n",
    "    for i in range(5):\n",
    "        res = 2*res\n",
    "        #print(\"changing window scale\")\n",
    "        for w,h in res:\n",
    "            #print(\"changing window apect ratio\")\n",
    "            #print(w,h, sz, 0,sz[0] - w, stride, 0,sz[1] - h,stride )\n",
    "            point_set = set((x,y) for x in range(0,sz[0] - w, stride) for y in range(0,sz[1] - h,stride))\n",
    "            #print(\"point set:\")\n",
    "            #print(len(point_set))\n",
    "            for cx,cy in point_set:\n",
    "                area = (cx, cy, cx + w, cy + h)\n",
    "                area_set = area_set + [area]\n",
    "                #cropped_images = cropped_images + [ (composed_transform(image.crop(area)), area)]\n",
    "    #print(\"returning cropped areas\")\n",
    "    return area_set\n",
    "    #cropped_images = Variable(cropped_images)\n",
    "        #outputs = resnet18(cropped_images)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxiou_accept(rwindow, boxes):\n",
    "    minWindowSize = 60\n",
    "    if(rwindow[2] - rwindow[0] < minWindowSize or rwindow[3] - rwindow[1] < minWindowSize ):\n",
    "        return -1,0 \n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    xx1 = np.maximum(rwindow[0], x1[:])\n",
    "    yy1 = np.maximum(rwindow[1], y1[:])\n",
    "    xx2 = np.minimum(rwindow[2], x2[:])\n",
    "    yy2 = np.minimum(rwindow[3], y2[:])\n",
    "    \n",
    "    rwindow_area = (rwindow[3] - rwindow[1] + 1)*(rwindow[2] - rwindow[0] + 1)\n",
    "    # compute the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "    # compute the ratio of overlap\n",
    "    print(w, h, w*h, area[:], rwindow_area)\n",
    "    overlap = (w * h) / (area[:]+rwindow_area - (w * h))\n",
    "    #print(max(overlap))\n",
    "    val = np.max(overlap)\n",
    "    ix = np.argmax(overlap)\n",
    "    #print(\"val,ix\")\n",
    "    #print(val,ix)\n",
    "    #print(\"overlap\")\n",
    "    #print(overlap)\n",
    "    return (val, ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_class_ap(class_areas, objlist, threshold=0.3):  #objlist:np.array, class_areas: dict\n",
    "    obj_dict = {}\n",
    "    class_ap = np.zeros(num_of_classes)\n",
    "    class_bin = np.zeros(num_of_classes)\n",
    "    for i in range(len(objlist)):\n",
    "        if objlist[i][1] in obj_dict:\n",
    "            obj_dict[objlist[i][1]] = obj_dict[objlist[i][1]] + [objlist[i][0]]\n",
    "        else:\n",
    "            obj_dict[objlist[i][1]] = [objlist[i][0]]\n",
    "            class_bin[objlist[i][1]] = 1\n",
    "    \n",
    "    #print(\"obj_dict is:\")\n",
    "    #print(obj_dict)\n",
    "    for key in class_areas:\n",
    "        #print(\"key in class area is {}\".format(key))\n",
    "        y = class_areas[key][:,1]\n",
    "        idx = np.argsort(-y)\n",
    "        ap = 0.0\n",
    "        if key in obj_dict:\n",
    "            #print(\"key in obj_dict : {}\".format(key))\n",
    "            count = 0.0\n",
    "            for i in range(len(idx)):\n",
    "                #print(\"error is here\")\n",
    "                #print(maxiou_accept(class_areas[key][idx[i],0], np.array(obj_dict[key])))\n",
    "                (overlap, ix) = maxiou_accept(class_areas[key][idx[i],0], np.array(obj_dict[key]))  #ix for if u want to erase that true window\n",
    "                \n",
    "                if(overlap > threshold):\n",
    "                    count = count + 1\n",
    "                    ap = ap + count/(i+1)\n",
    "            ap = ap/len(idx)\n",
    "        class_ap[key] = ap\n",
    "    #class_ap = class_ap/(len(class_areas) + epsilon)\n",
    "    return (class_ap, class_bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def test(resnet18):\n",
    "#     # Write loops for testing the model on the test set\n",
    "#     # You should also print out the accuracy of the model\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "# #     a = test_pipe_dataset.__getitem__(4)\n",
    "# #     print(\"hey\")\n",
    "# #     print(a)\n",
    "# #     print(\"yo\")\n",
    "#     for images, objlists in test_pipe_loader:   #presently only 1 image\n",
    "       \n",
    "#         for image in images:\n",
    "            \n",
    "#             objlists[0] = np.array(objlists[0])\n",
    "#             print(objlists[0][0].tuple())\n",
    "#             print(\"poop\")\n",
    "#             print(objlists[0].tolist())\n",
    "#             #print(type())\n",
    "#             print(objlists[0].numpy.tolist())\n",
    "#             imshow(torchvision.utils.make_grid(image))\n",
    "#             image_tensor = image\n",
    "#             image = revert_transform(image)\n",
    "#             image.save(\"image/\" + \"test\" + \".png\",\"PNG\")\n",
    "#             sz = image.size\n",
    "#             area_set = sliding_window(sz)\n",
    "#             win_batch = list()\n",
    "#             output_area_map = list()\n",
    "#             area_batch = list()\n",
    "#             area_predictions = torch.LongTensor()\n",
    "#             area_score = torch.LongTensor()\n",
    "#             for area in area_set:\n",
    "#                 #win_batch = list()\n",
    "#                 im = image.crop(area)\n",
    "#                 im = composed_transform(im)\n",
    "#                 win_batch = win_batch + [im]\n",
    "#                 #area_batch = area_batch + [area]\n",
    "#                 if(len(win_batch)%window_batch_size == 0 or area == area_set[-1]):\n",
    "#                     windows = torch.cat(win_batch)\n",
    "#                     windows = windows.view(-1, 3, resnet_input, resnet_input)\n",
    "#                     del win_batch[:]\n",
    "#                     windows = Variable(windows)\n",
    "#                     if(use_gpu):\n",
    "#                         windows = windows.cuda()\n",
    "#                     outputs = resnet18(windows)\n",
    "#                     #print(outputs.cpu())\n",
    "#                     #print(torch.nn.functional.softmax(outputs.data))\n",
    "#                     #probability, predicted = torch.max(torch.nn.functional.softmax(outputs.data).data, 1)\n",
    "#                     probability, predicted = torch.max(outputs.data, 1)\n",
    "#                     #print(probability)\n",
    "#                     backs = (probability < window_threshold)\n",
    "#                     predicted[backs] = 0 # background.\n",
    "#                     #print(\"baba baba\")\n",
    "#                     #print(type(probability.cpu().long()))\n",
    "#                     #print(type(predicted.cpu()))\n",
    "#                     #print(type(area_predictions))\n",
    "#                     area_predictions = torch.cat([area_predictions, predicted.cpu()])\n",
    "#                     area_score = torch.cat([area_score, probability.cpu().long()])\n",
    "#                     #print(\"preditctions:\")\n",
    "#                     #print(predicted)\n",
    "#             class_areas = {}\n",
    "#             for i in range(len(area_predictions)):\n",
    "#                 if area_predictions[i] == 0:\n",
    "#                     continue\n",
    "#                 if area_predictions[i] in class_areas:\n",
    "#                     class_areas[area_predictions[i]] = class_areas[area_predictions[i]] + [(area_set[i], area_score[i])]\n",
    "#                 else:\n",
    "#                     class_areas[area_predictions[i]] = [(area_set[i], area_score[i])]\n",
    "#             print(class_areas)\n",
    "#             for key in class_areas:\n",
    "                \n",
    "#                 pick = non_maximum_supression( np.array([i[0] for i in class_areas[key]]) )\n",
    "#                 class_areas[key] = np.array(class_areas[key])\n",
    "#                 print(class_areas[key])\n",
    "#                 print(pick)\n",
    "#                 class_areas[key] = class_areas[key][pick]\n",
    "                \n",
    "#             print(class_areas)\n",
    "#             for key in class_areas:\n",
    "#                 print(\"class is:\")\n",
    "#                 print(key)\n",
    "#                 for area, score in class_areas[key]:\n",
    "#                     #img_crp = image.crop\n",
    "#                     image.crop(area).save(\"image/\"+str(area)+ \"_\"+ str(score)+ \"_\" + str(key) + \".png\",\"PNG\")\n",
    "            \n",
    "# #             for (ar, cl) in objlists[0]:\n",
    "# #                 image.crop(ar).save(\"image/\" + \"truth_\" + str(ar) + \"_\" + str(cl) + \".png\", \"PNG\")\n",
    "            \n",
    "#             ans_ap = find_class_ap(class_areas, objlists[0].numpy.tolist())\n",
    "#             print(ans_ap)\n",
    "#             break\n",
    "#         break\n",
    "#             #image_windows = Variable(cropped_images)\n",
    "#             #if(use_gpu):\n",
    "#             #    image_windows = image_windows.cuda()\n",
    "#             #outputs = resnet18(image_windows)\n",
    "            \n",
    "\n",
    "            \n",
    "#             #_, predicted = torch.max(outputs.data, 1)\n",
    "#             #total += labels.size(0)\n",
    "#             #correct += (predicted.cpu() == labels.cpu()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "def test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    count = 0\n",
    "    num_of_test_samples = len(test_pipe_dataset)\n",
    "    #for images, objlist in test_pipe_loader:   #presently only 1 image   \n",
    "    #for image in images:\n",
    "    class_bin = np.zeros(num_of_classes) + epsilon\n",
    "    class_map = np.zeros(num_of_classes)\n",
    "    for idx in range(num_of_test_samples):\n",
    "        idx = random.randint(0,num_of_test_samples)\n",
    "        os.system('mkdir image/images_' + str(idx) + '/' )\n",
    "        image , objlist = test_pipe_dataset.__getitem__(idx)\n",
    "        image.save(\"image/images_\" + str(idx) + '/' + \"test\" + \".png\",\"PNG\")\n",
    "        sz = image.size\n",
    "        area_set = sliding_window(sz)\n",
    "        win_batch = list()\n",
    "        output_area_map = list()\n",
    "        area_batch = list()\n",
    "        area_predictions = torch.LongTensor()\n",
    "        area_score = torch.LongTensor()\n",
    "        for area in area_set:\n",
    "            im = image.crop(area)\n",
    "            im = composed_transform(im)\n",
    "            win_batch = win_batch + [im]\n",
    "            if(len(win_batch)%window_batch_size == 0 or area == area_set[-1]):\n",
    "                windows = torch.cat(win_batch)\n",
    "                windows = windows.view(-1, 3, resnet_input, resnet_input)\n",
    "                del win_batch[:]   #free up space\n",
    "                windows = Variable(windows)\n",
    "                if(use_gpu):\n",
    "                    windows = windows.cuda()\n",
    "                outputs = resnet18(windows)\n",
    "                score, predicted = torch.max(outputs.data, 1)\n",
    "                backs = (score < window_threshold)\n",
    "                predicted[backs] = 0 # background.\n",
    "                area_predictions = torch.cat([area_predictions, predicted.cpu()])\n",
    "                area_score = torch.cat([area_score, score.cpu().long()])\n",
    "        class_areas = {}\n",
    "        for i in range(len(area_predictions)):\n",
    "            if area_predictions[i] == 0:\n",
    "                continue\n",
    "            if area_predictions[i] in class_areas:\n",
    "                class_areas[area_predictions[i]] = class_areas[area_predictions[i]] + [(area_set[i], area_score[i])]\n",
    "            else:\n",
    "                class_areas[area_predictions[i]] = [(area_set[i], area_score[i])]\n",
    "        #print(class_areas)\n",
    "        for key in class_areas:\n",
    "\n",
    "            pick = non_maximum_supression( np.array([i[0] for i in class_areas[key]]) )\n",
    "            class_areas[key] = np.array(class_areas[key])\n",
    "            #print(class_areas[key])\n",
    "            #print(pick)\n",
    "            class_areas[key] = class_areas[key][pick]\n",
    "\n",
    "        #print(class_areas)\n",
    "        \n",
    "        for key in class_areas:\n",
    "            #print(\"class is:\")\n",
    "            #print(key)\n",
    "            for area, score in class_areas[key]:\n",
    "                #img_crp = image.crop\n",
    "                image.crop(area).save(\"image/images_\" + str(idx) + '/'+str(area)+ \"_\"+ str(score)+ \"_\" + str(classes[key]) + \".png\",\"PNG\")\n",
    "\n",
    "        for (ar, cl) in objlist:\n",
    "            image.crop(ar).save(\"image/images_\" + str(idx) + '/' + \"truth_\" + str(ar) + \"_\" + str(classes[cl]) + \".png\", \"PNG\")\n",
    "        temp = class_areas\n",
    "        ans_ap, cls_bin = find_class_ap(class_areas, np.array(objlist))\n",
    "        class_bin += cls_bin\n",
    "        class_map += ans_ap\n",
    "        \n",
    "        print(\"count = \")\n",
    "        print(count)\n",
    "        print(\"ans_ap: for one image\")\n",
    "        print(ans_ap)\n",
    "        #print(cls_bin)\n",
    "        print(\"class map till this point , count is: \")\n",
    "        print(class_map)\n",
    "        #print(\"class_bin :\")\n",
    "        #print(class_bin)\n",
    "        count+=1\n",
    "        if(count%10 == 0):\n",
    "            break\n",
    "    class_map = class_map / class_bin\n",
    "    print(\"final class map is\")\n",
    "    print(class_map)\n",
    "    print(\"final class bin is\")\n",
    "    print(class_bin)\n",
    "        #break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] [164] [16400] [23275] 20301\n",
      "[101] [178] [17978] [81090] 20301\n",
      "count = \n",
      "0\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "[0] [0] [0] [6930] 10201\n",
      "[0] [70] [0] [6930] 20301\n",
      "[20] [42] [840] [6930] 20301\n",
      "[94] [141] [13254] [17750] 20301\n",
      "count = \n",
      "1\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.]\n",
      "[  0 101] [201 201] [    0 20301] [37599 29106] 20301\n",
      "[151  45] [201 201] [30351  9045] [37599 29106] 40401\n",
      "[ 0 82] [201 198] [    0 16236] [37599 29106] 40401\n",
      "[32 85] [156 138] [ 4992 11730] [37599 29106] 20301\n",
      "count = \n",
      "2\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.\n",
      "  0.  0.  0.]\n",
      "[173  47   0] [201 165 174] [34773  7755     0] [62626 40000 43757] 40401\n",
      "[80  0  0] [201 201 201] [16080     0     0] [62626 40000 43757] 20301\n",
      "[  0  19 128] [201 201 201] [    0  3819 25728] [62626 40000 43757] 40401\n",
      "[ 74 125  46] [201 201 201] [14874 25125  9246] [62626 40000 43757] 40401\n",
      "count = \n",
      "3\n",
      "ans_ap: for one image\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.60416667  0.          0.          0.          0.          0.        ]\n",
      "class map till this point , count is: \n",
      "[ 0.          0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          2.60416667  0.          0.          0.          0.          0.        ]\n",
      "[101] [101] [10201] [100710] 10201\n",
      "[50] [177] [8850] [58900] 40401\n",
      "count = \n",
      "4\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.          0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          2.60416667  0.          0.          0.          0.          0.        ]\n",
      "[ 70 185] [  0 101] [    0 18685] [ 6020 31717] 20301\n",
      "[ 56 113] [ 27 101] [ 1512 11413] [ 6020 31717] 20301\n",
      "[107] [139] [14873] [14873] 80601\n",
      "count = \n",
      "5\n",
      "ans_ap: for one image\n",
      "[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0. ]\n",
      "class map till this point , count is: \n",
      "[ 0.          0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.5         0.          0.          0.          0.\n",
      "  0.          2.60416667  0.          0.          0.          0.          0.        ]\n",
      "[101] [201] [20301] [105288] 20301\n",
      "count = \n",
      "6\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.          0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.5         0.          0.          0.          0.\n",
      "  0.          2.60416667  0.          0.          0.          0.          0.        ]\n",
      "count = \n",
      "7\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.          0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.5         0.          0.          0.          0.\n",
      "  0.          2.60416667  0.          0.          0.          0.          0.        ]\n",
      "[101 101] [201 201] [20301 20301] [ 64119 148428] 20301\n",
      "[164 201] [201 201] [32964 40401] [ 64119 148428] 40401\n",
      "[118 181] [201 201] [23718 36381] [ 64119 148428] 40401\n",
      "[201 381] [144 197] [28944 75057] [ 64119 148428] 80601\n",
      "count = \n",
      "8\n",
      "ans_ap: for one image\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.35333333  0.          0.          0.          0.          0.        ]\n",
      "class map till this point , count is: \n",
      "[ 0.      0.      0.      1.      0.      0.      0.      0.      0.      0.5\n",
      "  0.      0.      0.      0.      0.      2.9575  0.      0.      0.      0.\n",
      "  0.    ]\n",
      "[201] [201] [40401] [162175] 40401\n",
      "[101] [100] [10100] [162175] 10201\n",
      "count = \n",
      "9\n",
      "ans_ap: for one image\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "class map till this point , count is: \n",
      "[ 0.      0.      0.      1.      0.      0.      0.      0.      0.      0.5\n",
      "  0.      0.      0.      0.      0.      2.9575  0.      0.      0.      0.\n",
      "  0.    ]\n",
      "final class map is\n",
      "[ 0.          0.          0.          0.999999    0.          0.          0.\n",
      "  0.          0.          0.16666661  0.          0.          0.          0.\n",
      "  0.          0.42249994  0.          0.          0.          0.          0.        ]\n",
      "final class bin is\n",
      "[  1.00000000e-06   1.00000000e-06   1.00000100e+00   1.00000100e+00\n",
      "   1.00000000e-06   1.00000000e-06   1.00000000e-06   1.00000000e-06\n",
      "   1.00000000e-06   3.00000100e+00   1.00000000e-06   1.00000000e-06\n",
      "   1.00000100e+00   1.00000000e-06   1.00000000e-06   7.00000100e+00\n",
      "   1.00000000e-06   1.00000000e-06   1.00000000e-06   1.00000100e+00\n",
      "   1.00000100e+00]\n",
      "CPU times: user 4min 7s, sys: 48.4 s, total: 4min 55s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%time test(resnet18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
