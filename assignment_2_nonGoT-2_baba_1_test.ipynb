{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#added by me\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "import unicodedata\n",
    "from IPython import display\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "#end of added by me\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# Import other modules if required\n",
    "\n",
    "resnet_input = 224 #size of resnet18 input images\n",
    "use_gpu = True\n",
    "num_of_classes = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate =  0.005\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomWindow(width, height):\n",
    "    xa = random.randint(0,width)\n",
    "    xb = random.randint(0,width)\n",
    "    ya = random.randint(0,height)\n",
    "    yb = random.randint(0,height)\n",
    "    x1 = min(xa,xb)\n",
    "    x2 = max(xa,xb)\n",
    "    y1 = min(ya,yb)\n",
    "    y2 = max(ya,yb)\n",
    "    return (x1,y1,x2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxiou(rwindow, boxes, threshold=0.2):\n",
    "    minWindowSize = 60\n",
    "    if(rwindow[2] - rwindow[0] < minWindowSize or rwindow[3] - rwindow[1] < minWindowSize ):\n",
    "        return 0\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    xx1 = np.maximum(rwindow[0], x1[:])\n",
    "    yy1 = np.maximum(rwindow[1], y1[:])\n",
    "    xx2 = np.minimum(rwindow[2], x2[:])\n",
    "    yy2 = np.minimum(rwindow[3], y2[:])\n",
    "    # compute the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "    # compute the ratio of overlap\n",
    "    overlap = (w * h) / area[:]\n",
    "    #print(max(overlap))\n",
    "    if(max(overlap) < threshold):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def build_dataset():\n",
    "    # Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class voc_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform  = transform\n",
    "        self.map = list()\n",
    "        \n",
    "        if(train == True):\n",
    "            rd = root_dir + \"/VOCdevkit_train/VOC2007/JPEGImages\"\n",
    "            rd_annot = root_dir + \"/VOCdevkit_train/VOC2007/Annotations\"\n",
    "            \n",
    "        else:\n",
    "            rd = root_dir + \"/VOCdevkit_test/VOC2007/JPEGImages\"\n",
    "            rd_annot = root_dir + \"/VOCdevkit_test/VOC2007/Annotations\"\n",
    "        \n",
    "        class_count = np.zeros(num_of_classes)\n",
    "        for subdir, dirs, files in os.walk(rd):\n",
    "            for File in files:\n",
    "                xmlFile = File.split('.')[0] + '.xml'\n",
    "                xmlFileDest = rd_annot + '/' + xmlFile\n",
    "                tree = ET.parse(xmlFileDest)\n",
    "                root = tree.getroot()\n",
    "                sz = root.find('size')\n",
    "                width = int(sz.find('width').text)\n",
    "                height = int(sz.find('height').text)\n",
    "                \n",
    "                objinfile = list()\n",
    "                objlist = list()\n",
    "                for obj in root.iter('object'):\n",
    "                    objclass = obj.find('name').text\n",
    "                    classid = classes.index(objclass)\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    \n",
    "                    #if(train == True):\n",
    "                    self.map = self.map + [(File, (xmin, ymin, xmax, ymax), classid)] #objclass is a string name of class.\n",
    "                    objinfile = objinfile + [(xmin, ymin, xmax, ymax)]\n",
    "                    #else:\n",
    "                        #objlist = objlist + [((xmin, ymin, xmax, ymax), classid)]\n",
    "                    class_count[classid]+=1\n",
    "                if(train == True):\n",
    "                    rwindow = getRandomWindow(width, height)\n",
    "                    #print(rwindow)\n",
    "                    if( maxiou(rwindow, np.array(objinfile)) > 0):\n",
    "                        #print(classes[0])\n",
    "                        #add background image\n",
    "                        self.map = self.map + [(File, rwindow, 0)]\n",
    "                        class_count[0]+=1\n",
    "                #else:\n",
    "                 #   self.map = self.map + [(File, objlist)]\n",
    "                       \n",
    "        #print(\"min height and width\")\n",
    "        #print(h,w)\n",
    "        print(\"num of examples per class:\")\n",
    "        print(classes)\n",
    "        print(class_count)\n",
    "               \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.map)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "       # Begin\n",
    "        if(self.train == True):\n",
    "            rd = self.root_dir + \"/VOCdevkit_train/VOC2007/JPEGImages\"\n",
    "            rd_annot = self.root_dir + \"/VOCdevkit_train/VOC2007/Annotations\"\n",
    "            \n",
    "        else:\n",
    "            rd = self.root_dir + \"/VOCdevkit_test/VOC2007/JPEGImages\"\n",
    "            rd_annot = self.root_dir + \"/VOCdevkit_test/VOC2007/Annotations\"\n",
    "            #return (im, self.map[1])\n",
    "        img = Image.open(rd + '/' + self.map[idx][0])\n",
    "        area = self.map[idx][1] #(xmin, ymin, xmax, ymax)\n",
    "        img = img.crop(area)  #taking only the part specified by the (xmin, ymin, xmax, ymax)\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(img)\n",
    "        return (im, self.map[idx][2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of examples per class:\n",
      "('__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
      "[ 1060.   331.   418.   599.   398.   634.   272.  1644.   389.  1432.\n",
      "   356.   310.   538.   406.   390.  5447.   625.   353.   425.   328.\n",
      "   367.]\n",
      "num of examples per class:\n",
      "('__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
      "[    0.   311.   389.   576.   393.   657.   254.  1541.   370.  1374.\n",
      "   329.   299.   530.   395.   369.  5227.   592.   311.   396.   302.\n",
      "   361.]\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.ToTensor() ])\n",
    "composed_test_transform = transforms.Compose([\n",
    "                                         transforms.ToTensor() ])\n",
    "revert_transform = transforms.Compose([\n",
    "                                         transforms.ToPILImage() ])\n",
    "#transforms.RandomHorizontalFlip() was removed by me from above.\n",
    "train_dataset = voc_dataset(root_dir='.', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = voc_dataset(root_dir='.', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here\n",
    "if(torch.cuda.is_available() and use_gpu):\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Begin\n",
    "    x = list()\n",
    "    y = list()\n",
    "    trdata_batchsize = len(train_dataset)//batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 256 == 0: \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, trdata_batchsize, loss.data[0]))\n",
    "                x.append((epoch*trdata_batchsize) + 1+i)\n",
    "                y.append(loss.data[0])\n",
    "                plt.plot(x,y,color = 'red')\n",
    "                plt.title('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, trdata_batchsize, loss.data[0]))\n",
    "                plt.xlabel(\"Batch Number\")\n",
    "                plt.ylabel(\"Cross Entropy Loss\")\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "            #if i == 400:\n",
    "             #   break\n",
    "        #break    #to run just 1 epoch\n",
    "    plt.savefig('Loss_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the weights you got by training it earlier.\n",
    "resnet18.load_state_dict(torch.load('assignment2_1_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBAaRFIIIYTQIJJCCCGEBpEUQgghNPh/i4+qKjIba8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f19d1cfd630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the weights for future analysis\n",
    "torch.save(resnet18.state_dict(), 'assignment2_2_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load saved model\n",
    "#resnet18.load_state_dict(torch.load('assignment2_1_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        #doing this on cpu due to gpu memory leak.\n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 62 %\n",
      "CPU times: user 9min 11s, sys: 1min, total: 10min 11s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%time test_accuracy(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes,threshold = 0.3):\n",
    "    # \n",
    "    # if there are no boxes, return an empty list\n",
    "\tif len(boxes) == 0:\n",
    "\t\treturn []\n",
    " \n",
    "\t# if the bounding boxes integers, convert them to floats --\n",
    "\t# this is important since we'll be doing a bunch of divisions\n",
    "\tif boxes.dtype.kind == \"i\":\n",
    "\t\tboxes = boxes.astype(\"float\")\n",
    " \n",
    "\t# initialize the list of picked indexes\t\n",
    "\tpick = []\n",
    " \n",
    "\t# grab the coordinates of the bounding boxes\n",
    "\tx1 = boxes[:,0]\n",
    "\ty1 = boxes[:,1]\n",
    "\tx2 = boxes[:,2]\n",
    "\ty2 = boxes[:,3]\n",
    " \n",
    "\t# compute the area of the bounding boxes and sort the bounding\n",
    "\t# boxes by the bottom-right y-coordinate of the bounding box\n",
    "\tarea = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\tidxs = np.argsort(y2)\n",
    " \n",
    "\t# keep looping while some indexes still remain in the indexes\n",
    "\t# list\n",
    "\twhile len(idxs) > 0:\n",
    "\t\t# grab the last index in the indexes list and add the\n",
    "\t\t# index value to the list of picked indexes\n",
    "\t\tlast = len(idxs) - 1\n",
    "\t\ti = idxs[last]\n",
    "\t\tpick.append(i)\n",
    " \n",
    "\t\t# find the largest (x, y) coordinates for the start of\n",
    "\t\t# the bounding box and the smallest (x, y) coordinates\n",
    "\t\t# for the end of the bounding box\n",
    "\t\txx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "\t\tyy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "\t\txx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "\t\tyy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "\t\t# compute the width and height of the bounding box\n",
    "\t\tw = np.maximum(0, xx2 - xx1 + 1)\n",
    "\t\th = np.maximum(0, yy2 - yy1 + 1)\n",
    " \n",
    "\t\t# compute the ratio of overlap\n",
    "\t\toverlap = (w * h) / area[idxs[:last]]\n",
    " \n",
    "\t\t# delete all indexes from the index list that have\n",
    "\t\tidxs = np.delete(idxs, np.concatenate(([last],\n",
    "\t\t\tnp.where(overlap > threshold)[0])))\n",
    " \n",
    "\t# return only the bounding boxes that were picked using the\n",
    "\t# integer data type\n",
    "\treturn boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(image):\n",
    "    \n",
    "    # Begin\n",
    "    \n",
    "    stride = 5\n",
    "    #for image in images:\n",
    "    image = revert_transform(image)\n",
    "    #image = transforms.ToPILImage(image)\n",
    "    res = np.array([(40,80),(80,40),(56,56)])/2  #may wish to change to powers of 2.\n",
    "    res = res.astype(int)\n",
    "    sz = image.size\n",
    "    cropped_images = []\n",
    "    for i in range(3):\n",
    "        res = 2*res\n",
    "        for w,h in res:\n",
    "            #print(w,h, sz, stride)\n",
    "            for cx,cy in set((x,y) for x in range(0,sz[0] - w, stride) for y in range(0,sz[1] - h,stride)):\n",
    "                cropped_images = cropped_images + [ composed_transform(image.crop((cx, cy, cx + w, cy + h)))]\n",
    "    return cropped_images\n",
    "    #cropped_images = Variable(cropped_images)\n",
    "        #outputs = resnet18(cropped_images)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, objlists in test_loader:\n",
    "        for image in images:\n",
    "            cropped_images = sliding_window(image)\n",
    "            image_windows = Variable(cropped_images)\n",
    "            if(use_gpu):\n",
    "                image_windows = image_windows.cuda()\n",
    "            outputs = resnet18(image_windows)\n",
    "            \n",
    "\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 80 (224, 224) 5\n",
      "80 40 (224, 224) 5\n",
      "56 56 (224, 224) 5\n",
      "80 160 (224, 224) 5\n",
      "160 80 (224, 224) 5\n",
      "112 112 (224, 224) 5\n",
      "160 320 (224, 224) 5\n",
      "320 160 (224, 224) 5\n",
      "224 224 (224, 224) 5\n",
      "40 80 (224, 224) 5\n",
      "80 40 (224, 224) 5\n",
      "56 56 (224, 224) 5\n",
      "80 160 (224, 224) 5\n",
      "160 80 (224, 224) 5\n",
      "112 112 (224, 224) 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9083915b1c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time test(resnet18)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-022eebd1b894>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(resnet18)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjlists\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-d74a9ed20cbf>\u001b[0m in \u001b[0;36msliding_window\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mcimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcimage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mcomposed_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/torchvision/transforms.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/torchvision/transforms.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mfloat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m\"\"\"Casts this tensor to float type\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anubhav/environments/VR/pytorch/local/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
